diff --git a/drivers/infiniband/sw/rxe/rxe.c b/drivers/infiniband/sw/rxe/rxe.c
index ae466e72fc43..f56dca8a94f1 100644
--- a/drivers/infiniband/sw/rxe/rxe.c
+++ b/drivers/infiniband/sw/rxe/rxe.c
@@ -4,6 +4,8 @@
  * Copyright (c) 2015 System Fabric Works, Inc. All rights reserved.
  */
 
+#include <linux/moduleparam.h>
+
 #include <rdma/rdma_netlink.h>
 #include <net/addrconf.h>
 #include "rxe.h"
@@ -13,6 +15,34 @@ MODULE_AUTHOR("Bob Pearson, Frank Zago, John Groves, Kamal Heib");
 MODULE_DESCRIPTION("Soft RDMA transport");
 MODULE_LICENSE("Dual BSD/GPL");
 
+int max_pkt_per_ack = RXE_MAX_PKT_PER_ACK;
+module_param(max_pkt_per_ack, int, 0644);
+MODULE_PARM_DESC(max_pkt_per_ack, "max # pkts between setting ack_req per qp");
+
+int max_unacked_psns = RXE_MAX_UNACKED_PSNS;
+module_param(max_unacked_psns, int, 0644);
+MODULE_PARM_DESC(max_unacked_psns, "max # pkts sent without receiving an ack per qp");
+
+int inflight_skbs_per_qp_high = RXE_INFLIGHT_SKBS_PER_QP_HIGH;
+module_param(inflight_skbs_per_qp_high, int, 0644);
+MODULE_PARM_DESC(inflight_skbs_per_qp_high, "max # undestroyed pkts sent per qp before pause");
+
+int inflight_skbs_per_qp_low = RXE_INFLIGHT_SKBS_PER_QP_LOW;
+module_param(inflight_skbs_per_qp_low, int, 0644);
+MODULE_PARM_DESC(inflight_skbs_per_qp_low, "min # undestroyed pkts sent per qp before scheduling req");
+
+int rxe_ndelay = 0;
+module_param_named(ndelay, rxe_ndelay, int, 0644);
+MODULE_PARM_DESC(ndelay, "Number of nsec to delay after sending a pkt");
+
+int max_iterations = RXE_MAX_ITERATIONS;
+module_param(max_iterations, int, 0644);
+MODULE_PARM_DESC(max_iterations, "Max number of calls to task before yielding cpu");
+
+bool dump_qp_on_error = false;
+module_param(dump_qp_on_error, bool, 0644);
+MODULE_PARM_DESC(dump_qp_on_error, "Dump qp state when queue transitions to error");
+
 /* free resources for a rxe device all objects created for this device must
  * have been destroyed
  */
diff --git a/drivers/infiniband/sw/rxe/rxe.h b/drivers/infiniband/sw/rxe/rxe.h
index d8fb2c7af30a..b817124ed133 100644
--- a/drivers/infiniband/sw/rxe/rxe.h
+++ b/drivers/infiniband/sw/rxe/rxe.h
@@ -38,6 +38,14 @@
 
 #define RXE_ROCE_V2_SPORT		(0xc000)
 
+extern int max_pkt_per_ack;
+extern int max_unacked_psns;
+extern int inflight_skbs_per_qp_high;
+extern int inflight_skbs_per_qp_low;
+extern int rxe_ndelay;
+extern int max_iterations;
+extern bool dump_qp_on_error;
+
 #define rxe_dbg(fmt, ...) pr_debug("%s: " fmt "\n", __func__, ##__VA_ARGS__)
 #define rxe_dbg_dev(rxe, fmt, ...) ibdev_dbg(&(rxe)->ib_dev,		\
 		"%s: " fmt, __func__, ##__VA_ARGS__)
diff --git a/drivers/infiniband/sw/rxe/rxe_hw_counters.c b/drivers/infiniband/sw/rxe/rxe_hw_counters.c
index a012522b577a..a553635edff8 100644
--- a/drivers/infiniband/sw/rxe/rxe_hw_counters.c
+++ b/drivers/infiniband/sw/rxe/rxe_hw_counters.c
@@ -22,6 +22,12 @@ static const struct rdma_stat_desc rxe_counter_descs[] = {
 	[RXE_CNT_LINK_DOWNED].name         =  "link_downed",
 	[RXE_CNT_RDMA_SEND].name           =  "rdma_sends",
 	[RXE_CNT_RDMA_RECV].name           =  "rdma_recvs",
+	[RXE_CNT_WAIT_PSN].name		   =  "wait_psn",
+	[RXE_CNT_WAIT_SKB].name		   =  "wait_skb",
+	[RXE_CNT_WAIT_FENCE].name	   =  "wait_fence",
+	[RXE_CNT_WAIT_DEPTH].name	   =  "wait_depth",
+	[RXE_CNT_XMIT_HARD_FAIL].name	   =  "xmit_hard_fail",
+	[RXE_CNT_XMIT_HARD_STALL].name	   =  "xmit_hard_stall",
 };
 
 int rxe_ib_get_hw_stats(struct ib_device *ibdev,
diff --git a/drivers/infiniband/sw/rxe/rxe_hw_counters.h b/drivers/infiniband/sw/rxe/rxe_hw_counters.h
index 71f4d4fa9dc8..f693cedc85d2 100644
--- a/drivers/infiniband/sw/rxe/rxe_hw_counters.h
+++ b/drivers/infiniband/sw/rxe/rxe_hw_counters.h
@@ -26,6 +26,12 @@ enum rxe_counters {
 	RXE_CNT_LINK_DOWNED,
 	RXE_CNT_RDMA_SEND,
 	RXE_CNT_RDMA_RECV,
+	RXE_CNT_WAIT_PSN,
+	RXE_CNT_WAIT_SKB,
+	RXE_CNT_WAIT_FENCE,
+	RXE_CNT_WAIT_DEPTH,
+	RXE_CNT_XMIT_HARD_FAIL,
+	RXE_CNT_XMIT_HARD_STALL,
 	RXE_NUM_OF_COUNTERS
 };
 
diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index cd59666158b1..97ed77fa8219 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -350,7 +350,7 @@ static void rxe_skb_tx_dtor(struct sk_buff *skb)
 	int skb_out = atomic_dec_return(&qp->skb_out);
 
 	if (unlikely(qp->need_req_skb &&
-		     skb_out < RXE_INFLIGHT_SKBS_PER_QP_LOW))
+		     skb_out < inflight_skbs_per_qp_low))
		rxe_sched_task(&qp->send_task);
 
 	rxe_put(qp);
@@ -453,6 +453,7 @@ int rxe_xmit_packet(struct rxe_qp *qp, struct rxe_pkt_info *pkt,
 	kfree_skb(skb);
 	err = 0;
 done:
+	ndelay(rxe_ndelay);
 	return err;
 }
 
diff --git a/drivers/infiniband/sw/rxe/rxe_req.c b/drivers/infiniband/sw/rxe/rxe_req.c
index d8c41fd626a9..54fc9e02f0a5 100644
--- a/drivers/infiniband/sw/rxe/rxe_req.c
+++ b/drivers/infiniband/sw/rxe/rxe_req.c
@@ -46,6 +46,7 @@ static void req_retry(struct rxe_qp *qp)
 	unsigned int cons;
 	unsigned int prod;
 
+	rxe_dbg_qp(qp, "called\n");
 	cons = queue_get_consumer(q, QUEUE_TYPE_FROM_CLIENT);
 	prod = queue_get_producer(q, QUEUE_TYPE_FROM_CLIENT);
 
@@ -101,7 +102,7 @@ void rnr_nak_timer(struct timer_list *t)
 	struct rxe_qp *qp = from_timer(qp, t, rnr_nak_timer);
 	unsigned long flags;
 
-	rxe_dbg_qp(qp, "nak timer fired\n");
+	rxe_info_qp(qp, "nak timer fired\n");
 
 	spin_lock_irqsave(&qp->state_lock, flags);
 	if (qp->valid) {
@@ -444,7 +444,7 @@ static void rxe_init_roce_hdrs(struct rxe_qp *qp, struct rxe_send_wqe *wqe,
 		    (is_send || is_write_imm);
 	dst_qpn = (pkt->mask & RXE_DETH_MASK) ? wr->wr.ud.remote_qpn :
 					       qp->attr.dest_qp_num;
-	ack_req = is_end || (qp->req.noack_pkts++ > RXE_MAX_PKT_PER_ACK);
+	ack_req = is_end || (qp->req.noack_pkts++ > max_pkt_per_ack);
 	if (ack_req)
 		qp->req.noack_pkts = 0;
 
@@ -728,6 +729,7 @@ int rxe_requester(struct rxe_qp *qp)
 
 	if (rxe_wqe_is_fenced(qp, wqe)) {
 		qp->req.wait_fence = 1;
+		rxe_counter_inc(rxe, RXE_CNT_WAIT_FENCE);
 		goto exit;
 	}
 
@@ -741,15 +743,17 @@ int rxe_requester(struct rxe_qp *qp)
 
 	if (unlikely(qp_type(qp) == IB_QPT_RC &&
 		psn_compare(qp->req.psn, (qp->comp.psn +
-				RXE_MAX_UNACKED_PSNS)) > 0)) {
+				max_unacked_psns)) > 0)) {
 		qp->req.wait_psn = 1;
+		rxe_counter_inc(rxe, RXE_CNT_WAIT_PSN);
 		goto exit;
 	}
 
 	/* Limit the number of inflight SKBs per QP */
 	if (unlikely(atomic_read(&qp->skb_out) >
-		     RXE_INFLIGHT_SKBS_PER_QP_HIGH)) {
+		     inflight_skbs_per_qp_high)) {
 		qp->need_req_skb = 1;
+		rxe_counter_inc(rxe, RXE_CNT_WAIT_SKB);
 		goto exit;
 	}
 
@@ -762,8 +766,10 @@ int rxe_requester(struct rxe_qp *qp)
 	mask = rxe_opcode[opcode].mask;
 	if (unlikely(mask & (RXE_READ_OR_ATOMIC_MASK |
 			RXE_ATOMIC_WRITE_MASK))) {
-		if (check_init_depth(qp, wqe))
+		if (check_init_depth(qp, wqe)) {
+			rxe_counter_inc(rxe, RXE_CNT_WAIT_DEPTH);
 			goto exit;
+		}
 	}
 
 	mtu = get_mtu(qp);
diff --git a/drivers/infiniband/sw/rxe/rxe_task.c b/drivers/infiniband/sw/rxe/rxe_task.c
index 80332638d9e3..8cc0c3f21cc8 100644
--- a/drivers/infiniband/sw/rxe/rxe_task.c
+++ b/drivers/infiniband/sw/rxe/rxe_task.c
@@ -120,7 +120,7 @@ static void do_task(struct rxe_task *task)
 	spin_unlock_irqrestore(&task->lock, flags);
 
 	do {
-		iterations = RXE_MAX_ITERATIONS;
+		iterations = max_iterations;
 		cont = 0;
 
 		do {
diff --git a/drivers/infiniband/sw/rxe/rxe_req.c b/drivers/infiniband/sw/rxe/rxe_req.c
index bd426a7..d7343d7 100644
--- a/drivers/infiniband/sw/rxe/rxe_req.c
+++ b/drivers/infiniband/sw/rxe/rxe_req.c
@@ -799,6 +799,7 @@ static void fake_udp_send(struct rxe_qp *qp, struct rxe_send_wqe *wqe)
 
 int rxe_requester(struct rxe_qp *qp)
 {
+	struct rxe_dev *rxe = to_rdev(qp->ibqp.device);
 	struct rxe_pkt_info pkt;
 	struct sk_buff *skb;
 	struct rxe_send_wqe *wqe;
diff --git a/drivers/infiniband/sw/rxe/rxe_qp.c b/drivers/infiniband/sw/rxe/rxe_qp.c
index 214cc1f..749df84 100644
--- a/drivers/infiniband/sw/rxe/rxe_qp.c
+++ b/drivers/infiniband/sw/rxe/rxe_qp.c
@@ -15,6 +15,8 @@
 #include "rxe_queue.h"
 #include "rxe_task.h"

+void dump_qp(struct rxe_qp *qp);
+
 static int rxe_qp_chk_cap(struct rxe_dev *rxe, struct ib_qp_cap *cap,
                          int has_srq)
 {
@@ -497,6 +497,8 @@ void rxe_qp_error(struct rxe_qp *qp)
	unsigned long flags;

	spin_lock_irqsave(&qp->state_lock, flags);
+	if (dump_qp_on_error)
+		dump_qp(qp);
	qp->attr.qp_state = IB_QPS_ERR;

	/* drain work and packet queues */
@@ -815,3 +817,27 @@ void rxe_qp_cleanup(struct rxe_pool_elem *elem)

	execute_in_process_context(rxe_qp_do_cleanup, &qp->cleanup_work);
 }
+
+void dump_qp(struct rxe_qp *qp)
+{
+	rxe_info_qp(qp, "need_req_skb = %d", qp->need_req_skb);
+	rxe_info_qp(qp, "(req.wqe_index = %d)(req.psn = %d)(req.opcode = %d)",
+			qp->req.wqe_index, qp->req.psn, qp->req.opcode);
+	rxe_info_qp(qp, "(req.wait_fence = %d)(req.need_rd_atomic = %d)",
+			qp->req.wait_fence, qp->req.need_rd_atomic);
+	rxe_info_qp(qp, "(req.wait_psn = %d)(req.need_retry = %d)(req.wait_for_rnr_timer = %d)",
+			qp->req.wait_psn, qp->req.need_retry, qp->req.wait_for_rnr_timer);
+	rxe_info_qp(qp, "(req.noack_pkts= %d)", qp->req.noack_pkts);
+	rxe_info_qp(qp, "(comp.psn = %d)(comp.opcode = %d)(comp.timeout = %d)",
+			qp->comp.psn, qp->comp.opcode, qp->comp.timeout);
+	rxe_info_qp(qp, "(comp.timeout_retry = %d)(comp.started_retry = %d)(comp.retry_cnt = %d)",
+			qp->comp.timeout_retry, qp->comp.started_retry, qp->comp.retry_cnt);
+	rxe_info_qp(qp, "(comp.rnr_retry = %d)", qp->comp.rnr_retry);
+	rxe_info_qp(qp, "(resp.msn = %d)(resp.psn = %d)(resp.ack_psn = %d)",
+			qp->resp.msn, qp->resp.psn, qp->resp.ack_psn);
+	rxe_info_qp(qp, "(resp.opcode = %d)(resp.drop_msg = %d)(resp.goto_error = %d)",
+			qp->resp.opcode, qp->resp.drop_msg, qp->resp.goto_error);
+	rxe_info_qp(qp, "(resp.sent_psn_nak = %d)(resp.status = %d)(resp.aeth_syndrome = %d)",
+			qp->resp.sent_psn_nak, qp->resp.status, qp->resp.aeth_syndrome);
+
+}

